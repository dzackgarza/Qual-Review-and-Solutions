# Fall 2014

1. Parts
   1. $f:X \to Y$ is surjective $\iff \forall y\in Y,~\exists x\in X \suchthat y=f(x)$.
   2. $f:X \to Y$ is not injective $\iff \exists x_1\neq x_2 \in X \suchthat f(x_1) = f(x_2).$
   3. $f: (X, d_x) \to (Y, d_y)$ is uniformly continuous $\iff \forall \varepsilon \exists \delta(\varepsilon) \suchthat \forall x_1,x_2\in X,~ \quad d_x(x_1, x_2) \leq \delta \implies d_y(f(x_1), f(x_2)) \leq \varepsilon.$

      *(Note that $\delta$ can only depend on $\varepsilon$ and must work for all $x_1, x_2$ simultaneously.)*
   4. $f$ is not uniformly continuous $\iff \exists \varepsilon \suchthat \forall \delta, \exists x_1, x_2\in X \suchthat \quad d_x(x_1, x_2) \leq \delta ~\&~ d_y(f(x_1), f(x_2)) > \varepsilon.$

2. Base case: for $n=1$, we have $a_1 = 1 \leq a_2 = \frac{16} 3 \leq 10.$ Suppose this holds for $k < n$, then 
$$
a_{n-1} \leq a_n = \frac{a_{n-1}}{3} + 5 \implies 3a_{n-1} \leq a_{n-1} + 15 \implies a_{n-1} \leq \frac{15}{2}
$$

   and thus we have
$$
a_{n+1} = \frac{a_n}{3} + 5 = \frac{1}{3}(a_n + 15) \\ 
= \frac{1}{3}((\frac{a_{n-1}}{3} + 5) + 15) \\
= \frac{a_{n-1} + 60}{9} \\
\leq \frac{\frac{15}{2} + 60}{9} \\
= \frac{150}{18} \\
< \frac{180}{18} = 10,
$$

and $a_{n+1} \leq 10$. Moreover, note that the relation $a_{n+1} = \frac{a_n}{3} + 5$ can be rewritten as 
$$
a_n = 3a_{n+1} - 15, \\ 
a_{n-1} = 3a_n - 15.
$$ Using the inductive hypothesis $a_{n-1} \leq a_n$, we can thus write
$$
3a_n - 15 = a_{n-1} \leq a_n = 3a_{n+1} - 15,
$$

from which we get $3a_{n} - 15 \leq 3a_{n+1} - 15$ and thus $a_{n} \leq a_{n+1}$.

To compute $\lim_{n\to\infty}a_n$, perhaps there are easier ways, but we can just use generating functions. Note that the limit exists by the **Monotone Convergence Theorem**. Let $A(x) = \sum_{n=0}^\infty a_n x^n$ where $a_0 = 0$. Then applying the magic sauce, we have
$$\begin{align*}
a_n = \frac{1}{3}a_{n-1} + 5 &\implies \sum_{n=1}^\infty a_nx^n = \frac{1}{3}\sum_{n=1}^\infty a_{n-1}x^n + 5\sum_{n=1}^\infty x^n \\
&\implies A(x) - a_0 = \frac 1 3 xA(x) + 5\left( \frac 1 {1-x} - 1\right) \\
&\implies A(x)\left(1 - \frac x 3\right) = 5\left( \frac x {1-x}\right) \\
&\implies A(x) = 15\left(\frac 1 {3-x} \right)\left(\frac x {1-x} \right) \\
&\implies A(x) = \frac{15x}{(3-x)(1-x)} \\
&\implies A(x) = \frac{-\frac{45}{2}}{3-x} + \frac{\frac{15}{2}}{1-x} \\
&\implies A(x) = \frac 3 2 \left(-5 \left( \frac{1}{1-\frac x 3} \right) + 5\left( \frac 1 {1-x}\right) \right) \\
&\implies A(x) = \frac{15}{2} \sum_{n=0}^\infty \left(1 - \left( \frac 1 3\right)^n\right)x^n \\
&\implies a_n = \frac {15} 2 \left(1 - \left( \frac 1 3\right)^n\right)
\end{align*}$$

and so we find 
$$
\lim_{n\to\infty}a_n = \lim_{n\to\infty}\frac {15} 2 \left(1 - \left( \frac 1 3\right)^n\right) = \frac{15}{2}. \qed
$$

  > Alternatively: 
  $$
  a_{n+1} = \frac 1 3 a_n + 5 \implies \lim_{n\to\infty} a_{n+1} = \lim_{n\to\infty} \frac 1 3 a_n + 5 \\
  \implies L = \frac 1 3 L + 5 \implies \frac 2 3 L = 5 \implies L = \frac {15} 2
  $$

1. Parts
   1. Suppose $\exists M_g \suchthat \forall x,~ g(x) < M$. Then let $\varepsilon > 0$ be arbitrarily chosen; we want to show that there exists a $\delta$ such that $\abs{x} \leq \delta \implies \abs{f(x)g(x)} \leq \varepsilon$. Since $\lim_{x\to 0} f(x) = 0$, choose a $\delta_f$ such that $\abs{x} \leq \delta_f \implies \abs{f(x)} \leq \frac{\varepsilon}{M_g}$. So letting $\delta = \delta_f$, we have
$$
\abs{x} \leq \delta \implies \abs{f(x)g(x)}  = \abs{f(x)} \abs{g(x)} \leq {\frac{\varepsilon}{M_g}}\abs{g(x)} \leq \frac{\varepsilon}{M_g}M_g = \varepsilon. \qed 
$$ 
     1. Let $f(x) = x$ and $g(x) = \frac{1}{x}$. Note that $g(x)$ is unbounded in any neighborhood of 0, and $f(x)g(x) = 1 \not\to 0$.

1. Let $\vector w_i$ be the proposed new basis elements -- then $\theset{\vector w_i}$ will be a basis if it is linearly independent and spans $\RR^3$. Since there are already three vectors in this set, we only need to check that they are linearly independent.
  By definition, we have
  $$
  \theset{\vector e_i} \text{ is linearly independent} \iff \sum c_i \vector e_i = \vector 0 \implies \forall i, ~ c_i = 0.
  $$

    Furthermore, since $\theset{\vector v_i}$ is known to be a basis, we have
  $$
  \sum c_i \vector v_i = \vector 0 \implies \forall i, ~ c_i = 0.
  $$

    So suppose $\sum c_i \vector w_i = \vector 0$, we want to show that $c_i = 0$ for each $i$. (This will mean that $\theset{\vector w_i}$ is linearly independent.) 

    We can expand this in terms of $\vector v_i$ as follows:
  $$
  c_1 \vector w_1 + c_2 \vector w_2 + c_3 \vector w_3  = \vector 0\\ 
  \implies c_1 (\vector v_1 + \vector v_2)  + c_2(\vector v_1 + \vector v_2 + \vector v_3) + c_3(-\vector v_2 + 2\vector v_3)  = \vector 0\\
  \implies c_1 \vector v_1 + (c_1+c_2+c_3) \vector v_1 + (-c_2 + 2c_3) \vector v_3  = \vector 0
  $$

    And using the fact that $\vector v_i$ is linearly independent, each coefficient of $\vector v_i$ here must be zero, and we arrive at the following system of equations:
  $$
  \begin{array}{lll}
  c_1 &&       &&       &=& 0 \\
  c_1 &+& c_2  &+& c_3  &=& 0 \\
      && -c_2 &+& 2c_3 &=& 0 \\
  \end{array}
  $$

    which can be rewritten as the matrix equation
  $$
  A\vector c 
  = \left[ \begin{array} { l l l } { 1 } & { 0 } & { 0 } \\ { 1 } & { 1 } & { 1 } \\ { 0 } & { - 1 } & { 2 } \end{array} \right]
  \left[\begin{array}{l}c_1 \\ c_2 \\ c_3 \end{array}\right] 
  = \vector 0
  $$

    and thus $\vector w_i$ will be linearly independent precisely if $A\vector c = \vector 0$ has only the trivial solution $\vector c = \vector 0$, which is precisely when $A$ has full rank, which happens iff $\det A \neq 0$. A quick calculation shows that $\det A = 3 \neq 0$, and so we are done. $\qed$

2. We first note that we can rewrite the equation of the region to obtain something more familiar: $x^2 + y^2 = 2x \implies (x-1)^2 + y^2 = 1$, which is a translated circle. Integrating over this region will be easy compared to the line integral, so we apply Green's theorem:
$$
\int_C xe^x ~dx + ye^y +x^2 ~dy = \iint_D 2x ~dA.
$$

    We can parameterize this region as 
  $$
  D = \theset{x^2+y^2-2x = 0 \suchthat (x,y) \in \RR^2, y \geq 0} 
  = \theset{(r(1+ \cos\theta), r\sin\theta) \suchthat \theta \in [0, \pi), r\in [0, 1]}.
  $$ 

    Noting that $dA = r~dr~d\theta$, we can then integrate
  $$
  \iint_D 2x ~dA 
  = \int_0^{\pi} \int_0^1 2(r(1 + \cos\theta)) r ~dr ~d\theta \\
  = 2\int_0^{\pi} \int_0^1 r^2(1+\cos \theta) ~dr ~d\theta \\
  = 2\int_0^{\pi} \frac 1 3 r^3(1+\cos \theta) \bigg\rvert_0^1  ~d\theta\\
  = \frac 2 3 \int_0^{\pi} (1+\cos \theta)  ~d\theta\\
  = \frac 2 3 ( \theta + \sin \theta) \bigg\rvert_0^\pi  \\
  = \frac 2 3 [(\pi + 0) - (0 + 0)]
  = \frac 2 3 \pi. \qed 
  $$

1. Parts
   1. If $A$ has two distinct eigenvalues, we will have $A = PDP\inv$ where $P$ is the matrix of eigenvectors and $D$ has eigenvalues on the diagonal. We can compute the characteristic polynomial
  $$
  p_\chi(x) = x^2 - (\Tr A)x + \det A = x^2 - 7x + 6 = (x-6)(x-1),
  $$

      and so $\spec(A) = \theset{6,1}$. Computing the kernel of $A-\lambda I$ for each of these yields
  $$
  \vector v_1 = \left[ \begin{array} { c } { 1 } \\ { - 2 } \end{array} \right],
  \vector v_2 = \left[ \begin{array} { c } { 2 } \\ { 1 } \end{array} \right],
  $$

      And so we can write and check
  $$
  P = \left[ \begin{array} { c c } { 1 } & { 2 } \\ { - 2 } & { 1 } \end{array} \right] \\
  D = \left(\begin{array}{rr}
    6 & 0 \\
    0 & 1
    \end{array}\right) \\
  $$

      We can compute $PP^T = \mathrm{diag}(5,5)$, so $P$ can be made orthogonal by replacing $P$ with $(1/\sqrt 5) P$. With this replacement, a quick computation shows that $PDP^T = A$.

   1. We will use the fact that $A = PDP\inv$ where since $A$ is symmetric and $P$ is orthogonal. We can write
   $$
   \inner{A\vector x}{\vector x} = \vector x^T A^T \vector x = \vector x^T (PDP^T)^T \vector x 
   = (P^T \vector x)^T D (P^T\vector x) = \vector y^T D \vector y = \inner{\vector y}{ D\vector y},
   $$
   where $\vector x\in S^2 \implies P^T\vector x \definedas \vector y \in S^2$ since $P^T$ is both orthogonal and full-rank (and thus a bijection $S^2 \selfmap$).

      We can now expand 
    $$
    \inner{\vector y}{D \vector y} = \sum_{i=1}^2 y_i \lambda_i y_i = \sum_{i=1}^2 \lambda_i y_i^2
    $$

      We now note that we can take $\vector y = \thevector{0, 1}$, in which case $D\vector y = \thevector{0, \lambda_2} = \thevector{0, 1}$ and thus $\inner{\vector y}{D \vector y} = 1$ is a candidate minimum.

      We can write this as the constrained optimization problem
      $$
      \text{Minimize } f(y_1, y_2) = 6y_1^2 + 1 y_2^2\\
      \text{subject to } g(y_1, y_2) = y_1^2 + y_2^2 = 1
      $$

      where we note that this constraint is equivalent to the original $\norm{\vector y} = \sqrt{y_1^2 + y_2^2} = 1$.

      This can be approached with Lagrange multipliers, i.e. looking at where $\nabla f = \lambda \nabla g$. This yields
      $$
      \thevector{12y_1, 2y_2} = \lambda \thevector{2y_1, 2y_2} \implies \\
      6y_1 = \lambda y_1, ~ y_2 = \lambda y_2.
      $$

      The second condition forces $y_2 \in \theset{0,1}$, and solving for $\lambda$ in this expression yields $\lambda = 1$ and so the first condition forces $y_1 = 0$. This leaves only one possibility, $\vector y = \thevector{0, 1}$, which is indeed the candidate from above. Thus the minimum value is 1. $\qed$
 

2. We need to show that $R$ is reflexive, transitive, and symmetric.
   1. Reflexive: this would say that $x\sim x \iff x^2-4x = x^2-4x$, which is true.
   2. Transitive: suppose $x\sim y$ and $y\sim z$, we want to show $x\sim z$. But we have
   $$
   x^2 - 4x = y^2-4y ~\&~ y^2-4y = z^2-4z \implies x^2-4x = y^2-4y = z^2-4z
   $$
   1. We want to show $x\sim y \implies y \sim x$, which follows because $x^2-4x = y^2-4y \iff y^2-4y = x^2-4x$.
   2. The equivalence classes:
   $$\begin{align*}
   x^2-4x &=  0:  &\theset{0, 4}\\
   x^2-4x &= -3: &\theset{1,3} \\
   x^2-4x &= -4: &\theset{2} \\
   x^2-4x &=  5: &\theset{5}
   \end{align*}$$

3. A function $f: \RR^2 \to \RR$ is totally differentiable at $\vector x$ if there exists a linear map $D: \RR^2 \to \RR$ such that 
  $$f(\vector x + \vector h)- f(\vector x) = D(\vector x)\vector h + \varepsilon(\vector h) \text{ where } \frac{\varepsilon(\vector h)}{\norm{\vector h}} \to 0.
  $$

    Equivalently, it is when the following limit exists and satisfies
    $$
    \lim_{\vector h \to \vector 0} \abs{\frac{f(\vector x + \vector h) - f(\vector x) - D(\vector x)\vector h}{\norm{\vector h}}} = 0,
    $$

    in which case we write $f' = D$, or in this special case, $f' = \nabla f$.

    Substituting in $\vector x = \vector 0$, being differentiable at zero requires that $\exists D$ such that
    $$
    f(\vector h) - f(\vector 0) = D(\vector 0)\vector h + \varepsilon(\vector h), ~\frac{\varepsilon(\vector h)}{\norm{\vector h}} \to 0, 
    \\ \text{ or } \\ 
    \lim_{\vector h \to \vector 0} \abs{\frac{f(\vector h) - f(\vector 0) - D(\vector 0)\vector h}{\norm{\vector h}}} = 0.
    $$

    We'll make use the mean value theorem. The general statement is
    $$
    f' \text{ exists and continuous on } (a,b)
    \implies \exists A \in [a,b] \suchthat \\
    f(b) - f(a) = f'(c)(b-a)
    $$
    and here we'll use
    $$
    f_x \text{ exists and continuous on } (0, t) 
    \implies \exists A \in [0, t] \suchthat \\ 
    f(t, y) - f(0, y) = f_x(A, y)(t - 0) = tf_x(A, y) \\
    \implies \color{purple} f(t, y) - f(0, y) = tf_x(A, y)
    $$
    where moreover 
    $$
    \lim_{t\to 0} f_x(A, y) = f_x(\lim_{t\to 0} A, y) = f(0, y)
    $$ 
    since $f_x$ was assumed to be continuous.

    We'll also use the "best linear approximation" definition of differentiability. In general, it states
    $$
    f' \text{ exists on } (a,b) \\
    \implies f(x) = f(a) + f'(a)(x-a) + o(x-a),~ \frac{o(x-a)}{x-a} \to 0 \\
    \implies f(x) - f(a) = f'(a)(x-a) + o(x-a),~ \frac{o(x-a)}{x-a} \to 0 \\
    \implies f(b) - f(a) = f'(a)(b-a) + o(b-a),~ \frac{o(b-a)}{b-a} \to 0
    $$
    and here we'll take $a=0, b=t$, so $x-a = x$ and $b-a = t$, and use the variant
    $$
    f_y \text{ exists on } (0, t) \\ 
    \implies f(x, t) - f(x, 0) = f_y(x, 0)(t - 0) + o(t),~ \frac{o(t)}{t} \to 0 \\
    \implies \color{green}f(x, t) - f(x, 0) = tf_y(x,0) + o(t), ~\frac{o(t)}{t} \to 0.
    $$

    We can then write
    $$\begin{align*}
    f(h_1, h_2) - f(0, 0)
    &= {\color{green}f(h_1, h_2) - f(h_1, 0)} + {\color{purple}f(h_1, 0) - f(0, 0)} \\
    &= {\color{green}h_2f_y(0, 0) + o(h_2)} + {\color{purple}h_1f_x(A, 0)}\\
    &= (h_1 f_x(0, 0) - {\color{red}h_1 f_x(0, 0)}) + {\color{purple}h_1f_x(A, 0)} + {\color{green}h_2f_y(0, 0) + o(h_2)} \\
    &= h_1 f_x(0, 0) + {\color{green}h_2 f_y(0, 0)} + h_1({\color{purple}f_x(A,0)} - {\color{red}f_x(0,0)}) + {\color{green}o(h_2)}
    \end{align*}$$

    Now, since $f_x$ is continuous, ${\color{purple}f_x(A,0)} - {\color{red}f_x(0,0)} \to 0$ and so $h_1(f_x(A,0) - f_x(0,0)) = o(h_1)$. We thus have
    $$\begin{align*}
    f(h_1, h_2) - f(0, 0)
    &= h_1 f_x(0, 0) + h_2 f_y(0, 0) + o(h_1) + o(h_2) \\
    &= h_1 f_x(0, 0) + h_2 f_y(0, 0) + o(\norm{\vector h}) \\ \\
    &\implies 
    f(h_1, h_2) - f(0, 0) - h_1 f_x(0, 0) - h_2 f_y(0, 0) = o(\norm{\vector h})
    \end{align*}$$

    and so if we define 
    $$D(\vector h) = h_1 f_x(0, 0) + h_2 f_y(0, 0),
    $$ 
    
    we can take absolute values to find that 
    $$
    \abs{f(\vector h) - f(\vector 0) - D(\vector h)} = o(\norm{\vector h}) \\
    \text{ and so } \\
    \lim_{\vector h \to \vector 0} \abs{\frac{f(\vector h) - f(\vector 0) - D(\vector h)}{\norm{\vector h}}} = 0
    $$

    which forces $f$ to be totally differentiable at $\vector 0$ with derivative $D$.

    
